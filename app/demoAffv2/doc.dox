/**
 * @ingroup icub_applications
 *
 * \defgroup icub_demoAffv2_app demoAffv2 

 *
 * \brief  Demonstration of affordance usage (2nd version)
 
In this demo we illustrate the use of affordances in simple imitation games. It is an example of how to use self-learned models of the world to interpret others actions and interact with them. 

\section demoAffv2 Introduction

Affordances relate robot's actions, object properties and the effects of the actions executed upon the objects. The core of the affordances is a Bayesian network that captures the previous robot experience. By using this network it is possible to infer based on the effects the best action to replicate them or to select the most appropriate object that will provide similar results. 

Details on how to learn this networks and how to use it can be found in \link http://www.robotcub.net/index.php/robotcub/more_information/deliverables/ RobotCub Deliverables \endlink and papers:

 - D4.1 : Experiments on Affordances
 \link http://www.robotcub.org/index.php/robotcub/content/download/1039/3660/file/RC_IST_AB_Deliverable_D4.1.pdf D4.1pdf
 \endlink
 - IEEE Transactions on robotics - Learning object affordances: from sensory-motor coordination to imitation,
(ref: L. Montesano, M. Lopes, A. Bernadino, J. Santos-Victor (2008) Learning object affordances: from sensory-motor coordination to imitation, \e IEEE Transactions \e on Robotics )

\section The Scenario

The application considers a scenario where a human interacts with the robot through objecs displaced in a table. In a first stage, the robot is engaged in a bottom up salience visual search behaviour, looking for objects (visual blobs) with distinct colors. When the human interacts with an object on the table, executing one action that results in an effect previously learned by the robot in its self exploration phase, the robot will try to imitate the effect (emulation). This may be done either on the same object or in a different one because to achieve the same effect, the robot may choose to use a different action.  

\section History

This demonstration is an evolutionn of the simpler demonstration demoaff but includes several improvements, mainly in what respects to modularization, automation and integration with other cognitive behaviors (e.g. the attention system).

\section Organization

This application consists on two main sub-applications with low-coupling, that are coordinated by a behavioural control module: (i) the attention system and (ii) the affordances system. The attention sub-system is described in 
\link http://eris.liralab.it/iCub/dox/html/group__icub__attentionDistributed.html. 
\endlink
The attention sybsystem is tuned to detect object with saturated and distinct colors, as well as image flicker. 
The affordances system is a breakdown of the modules described in 
\link http://eris.liralab.it/wiki/DemoAff demoaff
\endlink
with important changes with respect to the perceptual (object segmentation) and motor (manipulation) modules.   
A module denominated behaviourController is responsible for switching between a visual search behaviour (bottom uo salience attention system) the imitation behavior. The switching occurs when the attention of the robot is brought to one object in the table either by the robot's own visual search behaviour or guided by the human through action that atract robot's attention.

Both sybsystems and the behavior controller are lauched via scripts (.xml files) located in $ICUB_ROOT/app/demoAffv2.


\section dep_sec Dependencies
-# When connecting to a physical robot:
  - This module assumes \ref icub_iCubInterface "iCubInterface" is already running. The attention system operates the head. The affordances system operates the head, torso and arm.
  - Assumes that the cameras have been started, with corresponding camera calibration modules. The demonstration works with a resolution of 320x480. The attention system receives images from the right camera that should have a wide angle lens to provide an extended view field for the visual search (tests used a 2.8mm lens). The affordances subsystem receives images from the left camera that should have and a zoom lens of (tests used a 6mm lens) to provide good image descriptors of the objects' shape. A good calibration of the intrinsic parameters of the left camera is required to precisely locate the objects in cartesian coordinates. Also having a dark table, non-frontal illumination and a good color balance setting is essential for good object segmentation. Test have used the following settings for the left camera with good results.  Features: Shutter = 0.65, RED = 0.478, BLUE = 0.648: Features (adv): Saturation = 0.6. 

  
-# It has not been tested on the simulator.

- It depends on the following external libraries:
-# opencv
-# pnl

\section modules_sec Instantiated Modules

The attention sybsystem instantiates the following modules:
- \ref icub_salience "salience"
- \ref icub_egoSphere "egoSphere"
- \ref icub_attentionSelection "attentionSelection"
- \ref icub_controlGaze2 "controlGaze2"
- \ref icub_applicationGui "applicationGui"

The affordances subsystem instantiates the following modules:

- \ref icub_edisonSegmentation "edisonSegmentation"
- \ref icub_blobDescriptor "blobDescriptor" 
- \ref icub_effectDetector "effectDetector" 
- \ref icub_eye2world "eye2world"
- \ref icub_affordanceModule "affordanceModule"
- \ref icub_behaviorControl "behaviorControl"


\section Parameters
N/A

\section sec_configFiles Configuration Files
Configuration files are located in the ICUB_ROOT/app/demoAffv2/conf folder.

\section sec_example How to run the application

As mentioned previously, the application is composed by sub applications that can be run by executing the xml scripts in $ICUB_ROOT/app/demoAffv2. In the following subsections we describe in more detail how to setup and run the whole system.

\subsection sec_settingupmotors Setting up and running the motor control system.

The manipulation modules rely on the ICartesianControl interface, hence to compile it you need to 
enable the module icubmod_cartesiancontrollerclient in CMake while compiling the iCub repository, 
and to make this switch visible you have to tick the USE_ICUB_MOD just before.

To enable the cartesian features on the PC104, you have to launch iCubInterface pointing to the 
iCubInterfaceCartesian.ini file. Just afterwards you need to launch the solvers through the 
Application Manager XML file located under $ICUB_ROOT/app/cartesianSolver/scripts. PC104 code must be 
compiled with option icubmod_cartesiancontrollerserver active.


\subsection sec_settingupcameras Setting up and running the Cameras

Adjust and install the camera_calib.xml.template from $ICUB_ROOT/app/default/scripts for your robot. 
Adjust everything tagged as 'CHANGE' (nodes, contexts, config files). Use camCalibConfig to determine the 
calibration parameters. 
Start the cameras_calib.xml application using the manager.py script. 
camera_calib.xml starts the cameras, framegrabber GUIs, image viewers and 
camera calibration modules.
Check that you obtain the calibrated images. 

In the framegrabber GUI for the left camera select the Features tab and set:
Shutter = 0.65, RED = 0.478, BLUE = 0.648.
In the Features (adv) tab set: 
Saturation = 0.6. 


\subsection sec_settingupattention Setting up and running the Attention subsystem

Adjust and install the attention.xml.template from $ICUB_ROOT/app/demoAffv2/scripts. Adjust 
everything tagged as 'CHANGE' (mainly nodes). Make sure you compiled the Qt user interface at 
least on your local machine used for graphical interaction. This subpplication depends on the 
camera_calib.xml (cameras and camera calibration ports) and on the icub/head motor controlboard 
(dependencies are listed by the manager.py). 
Start the attention.xml application using the manager.py script. Check that all modules and ports 
are up and ready. You should see now the Qt user interface (applicationGui) on the machine 
you configured it to run on. You can then either connect all the listed connections shown in 
the manager.py user interface or use the applicationGui to connect them one by one. Note: You need 
to initialize the salience filter user interface by pressing the button 'initialize', this loads user 
interface controls for the available filters the salience module is running. Set at least one filter 
weight to non-zero, to start the attention system. 

\subsection sec_settingupaffordances Setting up and running the Affordance Based Imitation subsystem

Adjust and install the affordance_imitation.xml.template from $ICUB_ROOT/app/demoAffv2/scripts. 
Adjust everything tagged as 'CHANGE' (mainly nodes). 

\author Luis Montesano, Manuel Lopes, Jonas Ruesch, Matteo Taiana, Giovanni Saponaro, Ugo Pattacini, Christian Wressnegger, Alexandre Bernardino.

Copyright (C) 2008 RobotCub Consortium

CopyPolicy: Released under the terms of the GNU GPL v2.0.

This file can be edited at \in app/demoAffv2/doc.dox
 *
 */

